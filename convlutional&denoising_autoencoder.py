# -*- coding: utf-8 -*-
"""Convlutional&Denoising autoencoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZfhe0l4W532vS8mbKnYVv-JnQk8wVqI
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
# %pylab inline
from tensorflow import keras

# Load the MNIST dataset from Keras
(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()

# Normalize the training data to be in the range [0, 1]
x_train = x_train / 255.0

# Normalize the test data to be in the range [0, 1]
x_test = x_test / 255.0

# Define the encoder model using convolutional layers
encoder = keras.models.Sequential([
    # Reshape the input into a 28x28x1 image (grayscale)
    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),
    # First convolutional layer with 16 filters, each of size 3x3, ReLU activation, and padding to maintain size
    keras.layers.Conv2D(16, kernel_size=(3, 3), padding="same", activation="relu"),
    # Max pooling layer with pool size 2x2 to downsample the spatial dimensions
    keras.layers.MaxPool2D(pool_size=2),
    # Second convolutional layer with 32 filters, each of size 3x3, ReLU activation, and padding
    keras.layers.Conv2D(32, kernel_size=(3, 3), padding="same", activation="relu"),
    # Max pooling layer
    keras.layers.MaxPool2D(pool_size=2),
    # Third convolutional layer with 64 filters, each of size 3x3, ReLU activation, and padding
    keras.layers.Conv2D(64, kernel_size=(3, 3), padding="same", activation="relu"),
    # Max pooling layer
    keras.layers.MaxPool2D(pool_size=2)
])

# Predict the encoded representation of a single test image and print its shape
encoder.predict(x_test[0].reshape((1, 28, 28))).shape

# Define the decoder model using convolutional transpose layers
decoder = keras.models.Sequential([
    # Convolutional transpose layer with 32 filters, each of size 3x3, stride 2, ReLU activation, and valid padding
    keras.layers.Conv2DTranspose(32, kernel_size=(3, 3), strides=2, padding="valid",
                                 activation="relu",
                                 input_shape=[3, 3, 64]),
    # Convolutional transpose layer with 16 filters, each of size 3x3, stride 2, ReLU activation, and same padding
    keras.layers.Conv2DTranspose(16, kernel_size=(3, 3), strides=2, padding="same",
                                 activation="relu"),
    # Convolutional transpose layer with 1 filter, size 3x3, stride 2, sigmoid activation, and same padding
    keras.layers.Conv2DTranspose(1, kernel_size=(3, 3), strides=2, padding="same",
                                 activation="sigmoid"),
    # Reshape the output into the original image shape (28x28)
    keras.layers.Reshape([28, 28])
])

# Stack the encoder and decoder models to create the stacked autoencoder
stacked_autoencoder = keras.models.Sequential([encoder, decoder])

# Compile the stacked autoencoder model
stacked_autoencoder.compile(
    # Use binary cross-entropy loss function for binary input data (normalized pixel values)
    loss="binary_crossentropy",
    # Use the Adam optimizer for efficient training
    optimizer='adam'
)

# Train the stacked autoencoder model on the training data
history = stacked_autoencoder.fit(
    # Input and target data are both x_train (input images) for reconstruction
    x_train, x_train,
    # Train for 10 epochs
    epochs=30,
    # Use x_test for validation during training
    validation_data=[x_test, x_test]
)

# Set the figure size for the plot
figsize(20, 5)

# Iterate over 8 examples from the test set
for i in range(8):
    # Plot the original image from the test set
    subplot(2, 8, i+1)
    # Make a prediction using the stacked autoencoder on the current test image
    pred = stacked_autoencoder.predict(x_test[i].reshape((1, 28, 28)))
    # Display the original image
    imshow(x_test[i], cmap="binary")

    # Plot the reconstructed image by the stacked autoencoder
    subplot(2, 8, i+8+1)
    # Display the reconstructed image
    imshow(pred.reshape((28, 28)), cmap="binary")

# Set the figure size for the plot
figsize(15, 15)

# Iterate over all filters in the last convolutional layer of the encoder
for i in range(8 * 8):
    # Plot each filter as a subplot in an 8x8 grid
    subplot(8, 8, i+1)
    # Display the weights (filters) of the convolutional layer
    imshow(encoder.layers[-2].weights[0][:, :, 0, i])

"""Denoising autoencoder
The last application of autoencoders we look at today are denoising autoencoders. You probably have no difficulty classifying the images below as 7's.
"""

import numpy as np

# Set the figure size for the plot
figsize(5, 10)

# Plot the original image from the test set
subplot(1, 2, 1)
imshow(x_test[0], cmap="binary")

# Plot the noisy version of the original image
subplot(1, 2, 2)
# Generate random noise and add it to the original image
noise = np.random.random((28, 28)) / 4
imshow(x_test[0] + noise, cmap="binary")

# Define the encoder model using dense (fully connected) layers
encoder = keras.models.Sequential([
    # Flatten the input images (28x28) into a 1D array of 784 elements
    keras.layers.Flatten(input_shape=[28, 28]),
    # Add a dense layer with 100 neurons and ReLU activation function
    keras.layers.Dense(100, activation="relu"),
    # Add another dense layer with 100 neurons and ReLU activation function
    keras.layers.Dense(100, activation="relu"),
    # Add a dense layer with 30 neurons and ReLU activation function to generate the latent space representation
    keras.layers.Dense(30, activation="relu")
])

# Define the decoder model using dense (fully connected) layers
decoder = keras.models.Sequential([
    # Add a dense layer with 100 neurons and ReLU activation function, input shape is 30 (latent space representation)
    keras.layers.Dense(100, activation="relu", input_shape=[30]),
    # Add another dense layer with 100 neurons and ReLU activation function
    keras.layers.Dense(100, activation="relu"),
    # Add a dense layer with 784 neurons (28*28) and sigmoid activation function to output pixel values in range [0, 1]
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    # Reshape the output into the original image shape (28x28)
    keras.layers.Reshape([28, 28])
])

# Combine the encoder and decoder models to create the stacked autoencoder
stacked_autoencoder = keras.models.Sequential([encoder, decoder])

# Compile the stacked autoencoder model
stacked_autoencoder.compile(
    # Use binary cross-entropy loss function for binary input data (normalized pixel values)
    loss="binary_crossentropy",
    # Use the Adam optimizer for efficient training
    optimizer='adam'
)

# Add random noise to the training and test data
x_train_noise = x_train + ((np.random.random(x_train.shape)) / 4)
x_test_noise = x_test + ((np.random.random(x_test.shape)) / 4)

# Display an example of the noisy training data
imshow(x_train_noise[0], cmap="binary")

#Train the stacked autoencoder model on the noisy training data
history = stacked_autoencoder.fit(
    # Input is the noisy training data (x_train_noise), target is the original clean training data (x_train)
    x_train_noise, x_train,
    # Train for 10 epochs
    epochs=20,
    # Use noisy test data (x_test_noise) for validation during training, with original clean test data (x_test) as target
    validation_data=[x_test_noise, x_test]
)

# Set the figure size for the plot
figsize(20, 5)

# Iterate over 8 examples from the noisy test set
for i in range(8):
    # Plot the noisy version of the original image from the test set
    subplot(2, 8, i+1)
    imshow(x_test_noise[i], cmap="binary")

    # Plot the reconstructed image by the stacked autoencoder using the noisy input
    subplot(2, 8, i+8+1)
    # Make a prediction using the stacked autoencoder on the current noisy test image
    pred = stacked_autoencoder.predict(x_test_noise[i].reshape((1, 28, 28)))
    # Display the reconstructed image
    imshow(pred.reshape((28, 28)), cmap="binary")